<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Emre Can Acikgoz</title>
  
  <meta name="author" content="Irwan Bello">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Emre Can Acikgoz</name>
              </p>
              <p>
                I'm a Staff Research Scientist at <a href="https://research.google/teams/brain/"> Google Brain</a>. I am currently running a resesearch team working on various ideas related to large language models and their applications.
								<HR>
              </p>
              <p>
								My current research focus is making large scale language models more efficient through a variety of methods such as sparsity and improved distributed partitioning algorithms
								[<a href="https://arxiv.org/abs/2101.03961">Switch Transformer</a>, <a href="">Improving General Distributed Sparsity</a>]. 
								<br>
								<br>
								I also work on a variety of topics in computer vision: multi-task learning for structured prediction 
								[<a href="https://arxiv.org/abs/2108.11353">MuST</a>], semi-supervised and data augmentation methods [<a href="https://arxiv.org/abs/2006.06882">Rethinking</a>, <a href="https://arxiv.org/abs/1805.09501">AutoAugment</a>, 
									<a href="https://arxiv.org/abs/1909.13719">RandAugment</a>, <a href="https://arxiv.org/abs/2012.07177">Copy-Paste</a>] and simple baselines [<a href="https://arxiv.org/abs/2103.07579">ResNet-RS</a>, 
										<a href="https://arxiv.org/abs/2107.00057">Detection-RS</a>].
								<br>
								<br>
								At Google Brain I was a research TL for <a href="https://blog.google/products/search/introducing-mum/">MuM</a>. 
								In the past I have worked extensively on <a href="https://cloud.google.com/automl">AutoML</a> through research and product, where my work on <a href="https://en.wikipedia.org/wiki/Neural_architecture_search">Neural Architecture Search (NAS)</a> was an impetus.
                <br>
								<HR>
                Prior to Google Brain, I worked in at the <a href="https://www.isi.edu/research_groups/nlg/home">Information Sciences Institute</a> with <a href="https://kevincrawfordknight.github.io/">Kevin Knight</a> and <a href="https://www.isi.edu/~marcu/">Daniel Marcu</a> on statistical machine translation.
                <br>
                <br>
								In the summer of 2015 I wrote a distributed <a href="https://github.com/isi-nlp/Zoph_RNN">GPU RNN Library</a> in C++/CUDA for machine translation and language modeling. 
								It implemented both data parallelism and pipeline parallelism.
								<HR>
                <br>
                (Last update: January 2023)
                <br>
                <br>

              </p>
              <p style="text-align:center">
                <a href="mailto:eacikhoz17@ku.edu.tr">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=6h_3H8AAAAJ&hl=en&authuser=1">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/emrecanacikgoz">Twitter</a> &nbsp/&nbsp
								<a href="https://www.linkedin.com/in/emrecanacikgoz/">LinkedIn</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/emrecan.jpeg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/emrecan.jpeg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
				
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Talks and Media</heading>
              <p>
								<a href="https://www.youtube.com/watch?v=ccBMRryxGog&t=771s&ab_channel=YannicKilcher">Yannic Kilcher's Podcast on Sparsity for Large Language Models</a>
								<br>
								<a href="https://www.youtube.com/watch?v=noZiFhq8GBM&ab_channel=TowardsDataScience">Towards Data Science Podcast on Sparsity for Large Language Models</a>
								<br>
								<a href="https://thedataexchange.media/efficient-scaling-of-language-models/">The Data Exchange Podcast on Efficiently Scaling Large Language Models</a>
								<br>
                <a href="https://www.youtube.com/watch?v=O5Rrv6BvdgE&t=1602s&ab_channel=SamuelAlbanie">ICCV 2019 Neural Architects Workshop Talk</a>
								<br>
								<a href="https://www.youtube.com/watch?v=XDtFXBYpl1w&list=PLkFD6_40KJIwTmSbCv9OVJB3YaO4sFwkX&index=23&t=85s&ab_channel=CALESG-EECS">UC Berkeley Lecture on Deep Reinforcement Learning</a>
								<br>
								<a href="https://www.youtube.com/watch?v=2pbvnxdaKaw&ab_channel=KUISAI"> Ko√ß University Lecture on Transformer Sparsity</a>
								<br>
								<a href="https://www.nytimes.com/2017/11/05/technology/machine-learning-artificial-intelligence-ai.html"> Featured in NYT article on AutoML</a>								
								<br>
								<a href="https://www.technologyreview.com/2017/05/17/151652/why-googles-ceo-is-excited-about-automating-artificial-intelligence/"> Technology Review Article on Google's AutoML/Neural Architecture Search</a>
              </p>
            </td>
          </tr>  
        </tbody></table>
				
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                Representative papers are <span class="highlight">highlighted</span>.
								The * denotes equal author contribution.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
					
				<tr>
					<td class="dash">
						<br>
						<HR>
						<p style="font-size:18px;">Recent</p>
						<HR>
						<br>
					</td>
				</tr>

        <tr bgcolor="#ffffd0">
          <td>
            <p>
              <a href="https://arxiv.org/abs/2202.08906">
              <b>ST-MoE: Designing Scalable and Transferable Sparse Expert Models</b>
              </a>
              <br> 
              <i> <strong>Barret Zoph*</strong>, Irwan Bello*, Sameer Kumar, Nan Du, Yanping Huang, Noam Shazeer, William Fedus*.</i>
							<br>
							[ArXiv 2022] <a href="https://www.youtube.com/watch?v=ccBMRryxGog&t=771s&ab_channel=YannicKilcher">[Yannic Kilcher's Tutorial]</a>
              <p></p>
              <p>
              Sparse Mixture of Experts models suffer from training instabilities and finetuning issues at scale.
              We design improved methods for modeling, pretraining and finetuning sparse models and successfully finetune the largest sparse encoder-decoder model ever trained. 
							State-of-the-art results on many NLP benchmarks like SuperGLUE and ARC Easy / ARC Challenge.
              </p>
            </p>
          </td>
        </tr>

        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <br>
              <p align="right"><font size="2">
                <a href="http://www.cs.berkeley.edu/~barron/">(website template credits)</a>
                </font>
              </p>
            </td>
          </tr>
        </table>

      </td>
    </tr>
  </table>
</body>

</html>